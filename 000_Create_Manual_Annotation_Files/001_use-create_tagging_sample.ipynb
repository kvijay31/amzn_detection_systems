{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e52ee6",
   "metadata": {},
   "source": [
    "# Purpose: \n",
    "\n",
    "The purpose of this notebook is to utilise the files created from the first notebook, apply the rules for the creation of xlsx files for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a5566c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (4.63.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (4.63.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (1.21.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.3/913.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.8 smart-open-6.3.0 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting spacy-language-detection\n",
      "  Downloading spacy_language_detection-0.2.1-py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy-language-detection) (3.5.3)\n",
      "Collecting langdetect==1.0.9\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from langdetect==1.0.9->spacy-language-detection) (1.16.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.21.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (8.1.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (65.6.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (6.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (4.63.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from spacy>=3.0.0->spacy-language-detection) (1.10.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->spacy>=3.0.0->spacy-language-detection) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0.0->spacy-language-detection) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-language-detection) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->spacy-language-detection) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->spacy-language-detection) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->spacy-language-detection) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->spacy>=3.0.0->spacy-language-detection) (2.1.1)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=7a8832e393183a17de08a79957c9daabab343bff855e2243efd7c5223fc98dad\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c4/16/af/1889804d8b7c0c041cadee8e29673a938a332acbf2865c70a1\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, spacy-language-detection\n",
      "Successfully installed langdetect-1.0.9 spacy-language-detection-0.2.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (65.6.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fasttext) (1.21.6)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=286957 sha256=ae717cf612b1fb579a8eebee8f3b97b2bfe06a6c71237bd67990ae3a4300fcb2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/fc/22/93/1a3d535655339964fd8936d807ec85da466303d545023d2139\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from langdetect) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting p_tqdm\n",
      "  Downloading p_tqdm-1.4.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.45.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (4.63.2)\n",
      "Requirement already satisfied: pathos>=0.2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (0.3.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from p_tqdm) (1.16.0)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos>=0.2.5->p_tqdm) (0.3.6)\n",
      "Building wheels for collected packages: p_tqdm\n",
      "  Building wheel for p_tqdm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for p_tqdm: filename=p_tqdm-1.4.0-py3-none-any.whl size=5383 sha256=9b09dff9affe48fe3d567e585eec4c72823b6dcac3889af78ff765cf746e91a1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a2/54/28/97311859e49a55634542c9c839e4341fa00117ef1ae7ad0ffe\n",
      "Successfully built p_tqdm\n",
      "Installing collected packages: p_tqdm\n",
      "Successfully installed p_tqdm-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install nltk \n",
    "!pip install spacy\n",
    "!pip install spacy-language-detection\n",
    "!pip install fasttext\n",
    "!pip install langdetect\n",
    "!pip install p_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4daa691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import spacy\n",
    "from spacy_language_detection import LanguageDetector\n",
    "from spacy.language import Language\n",
    "import fasttext\n",
    "from langdetect import detect\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from p_tqdm import p_imap\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f60a56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.sent_end_chars=(':','.',',',';')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) - set(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'elven', 'twelve' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "408b2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text: str):\n",
    "    \"\"\"\n",
    "    Clean contractions in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean contractions.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with contractions expanded and cleaned.\n",
    "    \"\"\"\n",
    "    text = contractions.fix(text).lower()\n",
    "    text = re.sub('can t', ' cannot ', text).strip()\n",
    "    text = re.sub(' ll ', ' will ', text).strip()\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    text = re.sub(r' [^\\w\\s] ', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd8992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cleaning - utility functions: text_cleaning.py\n",
    "def basic_cleaning(text:string, punctuation:bool) -> str:\n",
    "    \"\"\"\n",
    "    This function does some basic data cleaning.\n",
    "    \"\"\"\n",
    "    for a_sign in ['\\\\n', '\\\\t', '☐', '☒', '\\xa0', '●', '“', '”']:\n",
    "        text = text.replace(a_sign,\" \")\n",
    "\n",
    "    if punctuation:\n",
    "        for a_punc in string.punctuation:\n",
    "            text = text.replace(a_punc, \" \")\n",
    "    else:\n",
    "        text =  re.sub(', ', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub('\\s+',\" \", text).lower()\n",
    "    return text.strip() \n",
    "\n",
    "def split_into_sentences(text:str) -> list:\n",
    "    \n",
    "    # Handle period (.)\n",
    "    text = re.sub(r'([a-zA-Z)])\\.([a-zA-Z-])', r'\\1. \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\.([a-zA-Z-])\", r\"\\1. \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\.(\\d)\", r\"\\1. \\2\", text)\n",
    "    #.Handle exclamation mark (!)\n",
    "    text = re.sub(r'([a-zA-Z)])\\!([a-zA-Z-])', r'\\1! \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\!([a-zA-Z-])\", r\"\\1! \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\!(\\d)\", r\"\\1! \\2\", text)\n",
    "    #? # Handle question mark (?)\n",
    "    text = re.sub(r'([a-zA-Z)])\\?([a-zA-Z-])', r'\\1? \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\?([a-zA-Z-])\", r\"\\1? \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\?(\\d)\", r\"\\1? \\2\", text)\n",
    "    #:  # Handle colon (:)\n",
    "    text = re.sub(r'([a-zA-Z)])\\:([a-zA-Z-])', r'\\1: \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\:([a-zA-Z-])\", r\"\\1: \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\:(\\d)\", r\"\\1: \\2\", text)\n",
    "    #;  # Handle colon (:)\n",
    "    text = re.sub(r'([a-zA-Z)])\\;([a-zA-Z-])', r'\\1; \\2', text)\n",
    "    text = re.sub(r\"([\\d)])\\;([a-zA-Z-])\", r\"\\1; \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z)])\\;(\\d)\", r\"\\1; \\2\", text)\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_special_characters(text:str):\n",
    "    \"\"\"\n",
    "    Remove special characters from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which special characters will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with special characters removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Replace \"%\" with \"percent\"\n",
    "    text = re.sub(r\"%\", ' percent ', text)\n",
    "    # Remove special characters except alphanumeric characters, spaces, and certain punctuation marks\n",
    "    return re.sub(r\"[^A-Za-z0-9\\s!.,?]+\", '', text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text:string):\n",
    "    \"\"\"\n",
    "    Remove stop words from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which stop words will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stop words removed.\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    return text \n",
    "\n",
    "def clean_contractions(text: str):\n",
    "    \"\"\"\n",
    "    Clean contractions in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean contractions.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with contractions expanded and cleaned.\n",
    "    \"\"\"\n",
    "    text = contractions.fix(text).lower()\n",
    "    text = re.sub('can t', ' cannot ', text).strip()\n",
    "    text = re.sub(' ll ', ' will ', text).strip()\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    text = re.sub(r' [^\\w\\s] ', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def removing_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Remove numbers and certain characters from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which numbers and characters will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with numbers and characters removed.\n",
    "    \"\"\"\n",
    "    text = re.sub('(?<=\\d),(?=\\s)', '', text)\n",
    "    text = re.sub('(?<=\\d).(?=\\d)', '', text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', text)\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def removing_non_english(text: str):\n",
    "    \"\"\"\n",
    "    Remove non-English words from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which non-English words will be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with non-English words removed.\n",
    "    \"\"\"\n",
    "    text = ' '.join([word for word in text.split() if word in words.words()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lammatize_text(text: str):\n",
    "    \"\"\"\n",
    "    Lemmatize the words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to lemmatize.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with lemmatized words.\n",
    "    \"\"\"\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def stem_text(text: str):\n",
    "    \"\"\"\n",
    "    Perform stemming on the words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to perform stemming.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stemmed words.\n",
    "    \"\"\"\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_words(text: str, word_list: dict):\n",
    "    \"\"\"\n",
    "    Replace specified words in the text with corresponding replacements.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text in which words will be replaced.\n",
    "        word_list (dict): A dictionary mapping words to their replacements.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with replaced words.\n",
    "    \"\"\"\n",
    "    for word in list(word_list):\n",
    "        text = text.replace(word, word_list[word])\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_cleaner(text: str, punctuation: bool, special_char: bool, stop_words: bool, contractions: bool,\n",
    "                 numbers: bool, non_english: bool, lammatize: bool, stem: bool, replacements: dict):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the given text based on the specified cleaning options.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "        punctuation (bool): Whether to remove punctuation.\n",
    "        special_char (bool): Whether to remove special characters.\n",
    "        stop_words (bool): Whether to remove stop words.\n",
    "        contractions (bool): Whether to expand and clean contractions.\n",
    "        numbers (bool): Whether to remove numbers and certain characters.\n",
    "        non_english (bool): Whether to remove non-English words.\n",
    "        lemmatize (bool): Whether to lemmatize words.\n",
    "        stem (bool): Whether to perform stemming.\n",
    "        replacements (dict): A dictionary mapping words to their replacements.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    text = basic_cleaning(text, punctuation)\n",
    "    if replacements is not None:\n",
    "        text = replace_words(text, replacements)\n",
    "    if special_char:\n",
    "        text = remove_special_characters(text)\n",
    "    if contractions:\n",
    "        text = clean_contractions(text)\n",
    "    if numbers:\n",
    "        text = removing_numbers(text)\n",
    "    if non_english:\n",
    "        text = removing_non_english(text)\n",
    "    if lammatize:\n",
    "        text = lammatize_text(text)\n",
    "    if stem:\n",
    "        text = stem_text(text)\n",
    "    if stop_words:\n",
    "        text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def creating_bigrams(text: str):\n",
    "    \"\"\"\n",
    "    Create bigrams from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to create bigrams from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of bigrams.\n",
    "    \"\"\"\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    bigrams = []\n",
    "    for sentence in sentences:\n",
    "        bigrams.extend(list(nltk.bigrams(sentence.split())))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def creating_unigrams(text: str):\n",
    "    \"\"\"\n",
    "    Create unigrams from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to create unigrams from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unigrams.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "\n",
    "#### Detect Language\n",
    "\n",
    "\n",
    "class languate_detection_fasttext():\n",
    "    \"\"\"\n",
    "    Initialize the language detection model using FastText.\n",
    "\n",
    "    This class detects the language of the text using FastText's language detection model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = os.path.join('/home/ec2-user/SageMaker/amazon_thesis_detectron/utils', \"lid.176.bin\")\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def detect_language(self, text):\n",
    "        \"\"\"\n",
    "        Detect the language of the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to detect the language from.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the detected language and its confidence score.\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(text, k=1)  # returns top 2 matching languages\n",
    "        return predictions[0][0].replace('__label__', ''), predictions[1][0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5384b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews_rating_references(amazon_reviews:DataFrame, max_reviews = None) -> DataFrame:\n",
    "    '''\n",
    "    This function takes a DataFrame containing reviews, splits the review in sentences, looks for references to rating or reviews, and returns a new DataFrame.\n",
    "    If desired, maximum number of reviews can be set. \n",
    "    '''\n",
    "    amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n",
    "    if max_reviews:\n",
    "        print(f\"Process only {max_reviews} of {len(amazon_reviews)}\")\n",
    "        amazon_reviews = amazon_reviews[0:max_reviews]\n",
    "    else:\n",
    "        print(f\"Process all {len(amazon_reviews)}.\")\n",
    "\n",
    "    def processing_reviews(chunk:list):\n",
    "        amazon_review_cleaned = []\n",
    "        for review in chunk:\n",
    "            senteces = split_into_sentences(text = review['review_body'])\n",
    "            for sentence in senteces:\n",
    "                temp = {\"asin\": review[\"product_id\"], 'review_id': review[\"review_id\"], 'reviewer_id': review[\"account_link\"], 'date': review[\"review_date\"], \"full_review\": review[\"review_body\"] }\n",
    "                temp['sentence'] =  sentence\n",
    "                temp['text_clean'] = text_cleaner(text = sentence, non_english= False,  punctuation= True, special_char= True, stop_words= True, contractions= True, numbers= False, lammatize= True, stem = True, replacements= None)\n",
    "                temp['text_unigrams'] = creating_unigrams(temp['text_clean'])\n",
    "                temp['rating_reference'] =  1 if (\"rate\" in temp['text_unigrams']) else 0\n",
    "                temp['star_reference'] =  1 if (\"star\" in temp['text_unigrams']) else 0\n",
    "                temp['review_reference'] =  1 if (\"review\" in temp['text_unigrams']) else 0\n",
    "                amazon_review_cleaned.append(temp)\n",
    "                del temp\n",
    "        return amazon_review_cleaned\n",
    "\n",
    "    print(\"Start multi-processing!\")\n",
    "    chunk_size = 100\n",
    "    chunks = [amazon_reviews[i:i + chunk_size] for i in range(0, len(amazon_reviews), chunk_size)]\n",
    "    print(len(chunks))\n",
    "    multi_processing_output = [ x for x in p_imap(processing_reviews, chunks )]\n",
    "    print(len(multi_processing_output))\n",
    "\n",
    "    return DataFrame([item for sublist in multi_processing_output  for item in sublist]), multi_processing_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b88e23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "language_detection = languate_detection_fasttext()\n",
    "# language_detection.detect_language('hello my name is kennedy')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "039fa752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon_reviews_office_3.json.json',\n",
       " 'amazon_reviews_office_2.json.json',\n",
       " 'amazon_reviews_office_8.json.json',\n",
       " 'amazon_reviews_office_0.json.json',\n",
       " 'amazon_reviews_office_6.json.json',\n",
       " 'amazon_reviews_office_9.json.json',\n",
       " 'amazon_reviews_office_7.json.json',\n",
       " 'amazon_reviews_office_4.json.json',\n",
       " 'amazon_reviews_office_5.json.json',\n",
       " 'amazon_reviews_office_1.json.json']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to the processed files which have been processed from raw data json files\n",
    "path_to_processed_raw_file = '/home/ec2-user/SageMaker/amazon_thesis_detectron/000_Create_Manual_Annotation_Files/001_office_data_processed'\n",
    "os.listdir(path_to_processed_raw_file) # listing all the json files which have been converted to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db8f02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to save the files for manual tagging \n",
    "saved_folder_path= '/home/ec2-user/SageMaker/amazon_thesis_detectron/000_Create_Manual_Annotation_Files/office_tagged_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f537013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187344/187344 [00:04<00:00, 40321.42it/s]\n",
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 25961.\n",
      "Start multi-processing!\n",
      "260\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02480149269104004,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 260,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4802b6e3ddc446c68cba0f0951ca5917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179088/179088 [00:05<00:00, 35259.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 23373.\n",
      "Start multi-processing!\n",
      "234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02066516876220703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 234,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13dc3a3ac0846e99cfe45623259bd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170538/170538 [00:04<00:00, 38830.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 23880.\n",
      "Start multi-processing!\n",
      "239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014303445816040039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 239,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7956c3d9bfc3485bb7f530d666224f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157389/157389 [00:04<00:00, 37887.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 21258.\n",
      "Start multi-processing!\n",
      "213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.060903072357177734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 213,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac201ac1bd842a4b85e97557b95e904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186133/186133 [00:04<00:00, 38195.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 22992.\n",
      "Start multi-processing!\n",
      "230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03177762031555176,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 230,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96833d69afaf48edaf632d1a31e51506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177886/177886 [00:04<00:00, 38854.87it/s]\n",
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 24597.\n",
      "Start multi-processing!\n",
      "246\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.050316810607910156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 246,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eeec30192a43928097eba28f0b53d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178409/178409 [00:04<00:00, 40316.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 24064.\n",
      "Start multi-processing!\n",
      "241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04093790054321289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 241,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e61a2c0be54af2b60474e1ed594d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173587/173587 [00:04<00:00, 38851.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 23229.\n",
      "Start multi-processing!\n",
      "233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04137277603149414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 233,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844f0c737e3441d29818f0ac48a3dd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184357/184357 [00:04<00:00, 38791.32it/s]\n",
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 24963.\n",
      "Start multi-processing!\n",
      "250\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04039311408996582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 250,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c927523ee948eeb65894a73cf76e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189493/189493 [00:04<00:00, 40639.53it/s]\n",
      "/tmp/ipykernel_8212/904060701.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  amazon_reviews = amazon_reviews[(amazon_reviews[\"language\"] == 'en') ].sample(frac  =1,  random_state=1).to_dict(orient = \"records\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process all 24616.\n",
      "Start multi-processing!\n",
      "247\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03471636772155762,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 247,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d976f99928114af4a19e4ba0c350c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(path_to_processed_raw_file):\n",
    "    tqdm.pandas()\n",
    "        # Find the position of the first full stop\n",
    "    first_full_stop_index = file.find('.')\n",
    "\n",
    "    # Extract the substring before the first full stop\n",
    "    extracted_text = file[:first_full_stop_index]\n",
    "    # convert this to a function once check is complete: \n",
    "    amazon_reviews_0_df = pd.read_json(os.path.join(path_to_processed_raw_file, file)) # 1000 products \n",
    "    amazon_reviews_0_df.review_body = amazon_reviews_0_df.review_body.apply(lambda x : x.replace(\"\\n\", \" \").strip())\n",
    "    amazon_reviews_0_df.review_body = amazon_reviews_0_df.review_body.replace(\"\\s+\", \" \")\n",
    "    amazon_reviews_0_df.rating = amazon_reviews_0_df.rating.apply(lambda x: float(x.replace(\" out of 5 stars\", \"\")))\n",
    "    amazon_reviews_0_df['language']= amazon_reviews_0_df.review_body.progress_apply(lambda x: language_detection.detect_language(x)[0])\n",
    "    amazon_reviews = amazon_reviews_0_df.copy()\n",
    "    amazon_reviews= amazon_reviews[['review_id', 'account_link', 'review_date', 'rating',\n",
    "           'review_title', 'review_body', 'purchase_type', 'helpful_votes',\n",
    "           'product_id', 'overall_rating', 'purchase_type', 'review_body',\n",
    "           'language']]\n",
    "    amazon_reviews_df, multi_processing_output = clean_reviews_rating_references(amazon_reviews=amazon_reviews[amazon_reviews['rating'] <= 2 ])\n",
    "    amazon_reviews_df_temp = amazon_reviews_df[(amazon_reviews_df['rating_reference'] == 1) | (amazon_reviews_df['star_reference'] == 1) | (amazon_reviews_df['review_reference'] == 1)].reset_index(drop = True) \n",
    "    amazon_reviews_df_temp[\"sentence\"] = amazon_reviews_df_temp[\"sentence\"].apply(lambda x: ' ' + x)\n",
    "    amazon_reviews_df_temp.to_excel(os.path.join(saved_folder_path,f'{extracted_text}.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3bcf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"End of Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b9187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
