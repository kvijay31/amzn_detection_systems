{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: \n",
    "\n",
    "The purpose of the notebook is to further investifate Q&A and topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kartikvijay/opt/miniconda3/envs/kartik_env/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# USE-\n",
    "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "\n",
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is the text talking about misleading rating?\"\n",
    "question = 'is it talking about fake reviews?'\n",
    "# context = \"42 is the answer to life, the universe and everything\"\n",
    "# context = ' I dont know how this book received so many good reviews'\n",
    "# context = 'They said that it is for brother mfc6490, but it is not compatible. i believe that positive reviews here are fake. what a waste of money!'\n",
    "# context = 'BUYER BEWARE!!! I ordered theese and none of them have glue. Absolute junk.  Any good review on this product is likely fake'\n",
    "# context = 'Stapler has a 50% success rate.  The other 50% it jams, it doesn’t pass the staple all the way through, etc.  real garbage.  I didn’t know you could make a bad stapler.  Now I do.'\n",
    "context = 'The tape on these NEVER stays in the track. The tape is always coming off and has to be re positioned constantly.Not worth your time and after having bought 2 boxes I opted to throw the rest out rather than suffer thru using them any more. Total garbage. Dont believe all those phony reviews and save yourself from all this.'\n",
    "# This by far being one of the worst movies I have ever seen. What a horrible story line I was so bored. The cast stands outside for part of this movie, while people disappear, they argue. Then they move into the lodge and the rest of the cast disappears. Thats it thats the movie, no explanation, no climax , just a poorly written script . That is how it ends just with nothing that what they give you. I dont even understand the whole point of this story because there was none. I dont think anyone could ever give this a good review'\n",
    "# context = \"I bought this product based on the high rating, but it turned out to be completely different from what I expected. The rating must be misleading.\"\n",
    "# context = 'ratings are bad'\n",
    "input = f\"question: {question} context: {context}\"\n",
    "encoded_input = tokenizer([input],\n",
    "                             return_tensors='pt',\n",
    "                             max_length=512,\n",
    "                             truncation=True)\n",
    "output = model.generate(input_ids = encoded_input.input_ids,\n",
    "                            attention_mask = encoded_input.attention_mask)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'I am totally convinced the good reviews are fake. This by far being one of the worst movies I have ever seen. What a horrible story line I was so bored. The cast stands outside for part of this movie, while people disappear, they argue. Then they move into the lodge and the rest of the cast disappears. Thats it thats the movie, no explanation, no climax , just a poorly written script . That is how it ends just with nothing that what they give you. I dont even understand the whole point of this story because there was none. I dont think anyone could ever give this a good review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is relevant to the topic 'beguiling'.\n",
      "The sentence is relevant to the topic 'deceitful'.\n",
      "The sentence is relevant to the topic 'deceiving'.\n",
      "The sentence is relevant to the topic 'deceptive'.\n",
      "The sentence is relevant to the topic 'deluding'.\n",
      "The sentence is relevant to the topic 'delusive'.\n",
      "The sentence is relevant to the topic 'delusory'.\n",
      "The sentence is relevant to the topic 'fallacious'.\n",
      "The sentence is relevant to the topic 'false'.\n",
      "The sentence is relevant to the topic 'specious'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "# load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# loop over the topics and check if the sentence is relevant to each one\n",
    "# define the topics and sentence to check\n",
    "# topics = [\"artificial intelligence\", \"machine learning\", \"natural language processing\"]\n",
    "# sentence = \"Machine learning is a subfield of artificial intelligence.\"\n",
    "# topics = ['misleading reviews', 'misleading rating', 'fake rating', 'fake review']\n",
    "topics = ['beguiling',\n",
    "  'deceitful',\n",
    "  'deceiving',\n",
    "  'deceptive',\n",
    "  'deluding',\n",
    "  'delusive',\n",
    "  'delusory',\n",
    "  'fallacious',\n",
    "  'false',\n",
    "  'specious']\n",
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# topics = ['misleading', 'fake review']\n",
    "# sentence= 'just read it realms of layers etc i like it only one of a kind'\n",
    "# sentence=  'The tape on these NEVER stays in the track. The tape is always coming off and has to be re positioned constantly.Not worth your time and after having bought 2 boxes I opted to throw the rest out rather than suffer thru using them any more. Total garbage.'\n",
    "sentence = 'Ok I dont think the 5 star reviews on this product are honest. The set is extremely cheap. The case is flimsy and the compass is not well built. I doubt this will serve the needs of my son. I am generally very good at confirming 5 star reviews but fell for it hard this time around. Buyer beware!'\n",
    "for topic in topics:\n",
    "    # tokenize the sentence and topic using the tokenizer\n",
    "    inputs = tokenizer.encode_plus(sentence, topic, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    # use the model to generate a prediction for the relevance of the sentence to the topic\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "    # print the prediction (0 for not relevant, 1 for relevant)\n",
    "    if prediction == 1:\n",
    "        print(f\"The sentence is relevant to the topic '{topic}'.\")\n",
    "    else:\n",
    "        print(f\"The sentence is not relevant to the topic '{topic}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is relevant to the topic 'beguiling'.\n",
      "The sentence is relevant to the topic 'deceitful'.\n",
      "The sentence is relevant to the topic 'deceiving'.\n",
      "The sentence is relevant to the topic 'deceptive'.\n",
      "The sentence is relevant to the topic 'deluding'.\n",
      "The sentence is relevant to the topic 'delusive'.\n",
      "The sentence is relevant to the topic 'delusory'.\n",
      "The sentence is relevant to the topic 'fallacious'.\n",
      "The sentence is relevant to the topic 'false'.\n",
      "The sentence is relevant to the topic 'specious'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "# load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# loop over the topics and check if the sentence is relevant to each one\n",
    "# define the topics and sentence to check\n",
    "# topics = [\"artificial intelligence\", \"machine learning\", \"natural language processing\"]\n",
    "# sentence = \"Machine learning is a subfield of artificial intelligence.\"\n",
    "# topics = ['misleading reviews', 'misleading rating', 'fake rating', 'fake review']\n",
    "topics = ['beguiling',\n",
    "  'deceitful',\n",
    "  'deceiving',\n",
    "  'deceptive',\n",
    "  'deluding',\n",
    "  'delusive',\n",
    "  'delusory',\n",
    "  'fallacious',\n",
    "  'false',\n",
    "  'specious']\n",
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# topics = ['misleading', 'fake review']\n",
    "# sentence= 'just read it realms of layers etc i like it only one of a kind'\n",
    "# sentence = 'Utter garbage. Take the $5 to $7 you plan to spend on this \"tool\" and burn it if you feel the desire to make this purchase. You will never be able to make a reproachable line because a) the two arms move out of plane from each other, b) the screw assembly slips constantly, c) the screw assemble actually runs through a slot rather than a hole, meaning that even when the nut is now moving, you have a 1/4\" wiggle in the scribe line. The positive reviews HAVE TO BE fakes. This thing should not even be in a middle school math class.'\n",
    "# sentence=  'The tape on these NEVER stays in the track. The tape is always coming off and has to be re positioned constantly.Not worth your time and after having bought 2 boxes I opted to throw the rest out rather than suffer thru using them any more. Total garbage.'\n",
    "# sentence = 'I have experienced all of the failure modes listed in these reviews - the printer james often, and in unpredictable ways, the cartridges run out faster than you can believe, and the wireless drops off at the most inopportune times.  I have had to run to a local copy shop to make really critical prints when the cartridge ran out when I needed copies for an important early meeting.Customer service could not be more indifferent.  I actually came to Amazon today to read the reviews on other printers because I am just trashing my Samsung.  I was furious with HP over their ink prices; however, they are not looking so bad now.  Is there a reliablbe, cost efficient printer anywhere?  All those positive reviews for this model makes me question all positive reviews anywhere.I feel like such a sucker for buying this thing.'\n",
    "# sentence = 'Ok I dont think the 5 star reviews on this product are honest. The set is extremely cheap. The case is flimsy and the compass is not well built. I doubt this will serve the needs of my son. I am generally very good at confirming 5 star reviews but fell for it hard this time around. Buyer beware!'\n",
    "# sentence = 'I only hope that this was a quality assurance rather than a deliberate change of Avery product specifications. I have used these protectors for a at least 5 years and they were perfect. Now, I can barely squeeze in a single page and it buckles up (see photos by other reviewers). My last order now goes into garbage, just as I am looking for an alternative product.'\n",
    "# sentence = 'When I purchased these, it was advertised as 2011, which I needed. However, when I received them, they were for 2012. They have no contact info listed and when I reviewed these the first time, it wasnt accepted. I guess they dont like negative feedback. Secondary, I havent heard anything from them to replace .'\n",
    "# sentence = 'Colors are NOT as shown in picture… I received darker colors - just like some of the other reviewers.'\n",
    "sent = 'I cant find anywhere else to get customer support, so Im reporting my issue here as a 1 star review.  If E-Z Ink can help me get this resolved, I will adjust the review.  I purchased this toner cartridge for a Samsung M2885FW printer.  When I install it in the printer, the printer gives me an error message saying toner not compatible with printer.'\n",
    "# sentt= 'Incompatible. Instructions to override useless.  Sent back in recycle bag provided.  Printer manufacturers got you by the cajones.  I should give one star, but showed mercy.'\n",
    "# sentt = ' Docking 4 stars because there is not any support for the printer.'\n",
    "for topic in topics:\n",
    "    # tokenize the sentence and topic using the tokenizer\n",
    "    inputs = tokenizer.encode_plus(sentt, topic, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    # use the model to generate a prediction for the relevance of the sentence to the topic\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "    # print the prediction (0 for not relevant, 1 for relevant)\n",
    "    if prediction == 1:\n",
    "        print(f\"The sentence is relevant to the topic '{topic}'.\")\n",
    "    else:\n",
    "        print(f\"The sentence is not relevant to the topic '{topic}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the topic and sentence to check\n",
    "import requests\n",
    "topic = \"fake\"\n",
    "sentence = \"Machine learning is a subfield of artificial intelligence.\"\n",
    "\n",
    "# use the Merriam-Webster Thesaurus API to retrieve synonyms for the topic\n",
    "response = requests.get(f\"https://www.dictionaryapi.com/api/v3/references/thesaurus/json/{topic}?key=2c68e900-56f7-45f6-838b-aa6ee59469cb\")\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error retrieving synonyms for '{topic}': {response.content}\")\n",
    "    exit()\n",
    "\n",
    "synonyms = []\n",
    "data = response.json()[0]\n",
    "# for meta in data.get(\"meta\", []):\n",
    "#     if meta.get(\"syns\"):\n",
    "#         for syn in meta[\"syns\"][0]:\n",
    "#             synonyms.append(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'id': 'fake',\n",
       "  'uuid': '8573e37f-a56f-40c4-abf2-0b56689321a9',\n",
       "  'src': 'coll_thes',\n",
       "  'section': 'alpha',\n",
       "  'target': {'tuuid': '8d8e4817-db63-4cfa-a345-e0693e294ba5',\n",
       "   'tsrc': 'collegiate'},\n",
       "  'stems': ['fake'],\n",
       "  'syns': [['artificial',\n",
       "    'bogus',\n",
       "    'dummy',\n",
       "    'ersatz',\n",
       "    'factitious',\n",
       "    'false',\n",
       "    'faux',\n",
       "    'imitation',\n",
       "    'imitative',\n",
       "    'man-made',\n",
       "    'mimic',\n",
       "    'mock',\n",
       "    'pretend',\n",
       "    'sham',\n",
       "    'simulated',\n",
       "    'substitute',\n",
       "    'synthetic'],\n",
       "   ['bogus',\n",
       "    'counterfeit',\n",
       "    'false',\n",
       "    'forged',\n",
       "    'inauthentic',\n",
       "    'phony',\n",
       "    'queer',\n",
       "    'sham',\n",
       "    'snide',\n",
       "    'spurious',\n",
       "    'unauthentic'],\n",
       "   ['affected',\n",
       "    'artificial',\n",
       "    'assumed',\n",
       "    'bogus',\n",
       "    'contrived',\n",
       "    'factitious',\n",
       "    'false',\n",
       "    'feigned',\n",
       "    'forced',\n",
       "    'mechanical',\n",
       "    'mock',\n",
       "    'phony',\n",
       "    'plastic',\n",
       "    'pretended',\n",
       "    'pseudo',\n",
       "    'put-on',\n",
       "    'sham',\n",
       "    'simulated',\n",
       "    'spurious',\n",
       "    'strained',\n",
       "    'unnatural'],\n",
       "   ['artificial',\n",
       "    'backhanded',\n",
       "    'counterfeit',\n",
       "    'double',\n",
       "    'double-dealing',\n",
       "    'double-faced',\n",
       "    'feigned',\n",
       "    'hypocritical',\n",
       "    'insincere',\n",
       "    'Janus-faced',\n",
       "    'jive',\n",
       "    'left-handed',\n",
       "    'lip',\n",
       "    'mealy',\n",
       "    'mealymouthed',\n",
       "    'Pecksniffian',\n",
       "    'phony',\n",
       "    'phony-baloney',\n",
       "    'pretended',\n",
       "    'two-faced',\n",
       "    'unctuous']],\n",
       "  'ants': [['genuine', 'natural', 'real'],\n",
       "   ['authentic', 'bona fide', 'genuine', 'real', 'unfaked'],\n",
       "   ['artless',\n",
       "    'genuine',\n",
       "    'natural',\n",
       "    'spontaneous',\n",
       "    'unaffected',\n",
       "    'uncontrived',\n",
       "    'unfeigned',\n",
       "    'unforced'],\n",
       "   ['artless',\n",
       "    'candid',\n",
       "    'genuine',\n",
       "    'heartfelt',\n",
       "    'honest',\n",
       "    'sincere',\n",
       "    'undesigning',\n",
       "    'unfeigned']],\n",
       "  'offensive': False},\n",
       " 'hwi': {'hw': 'fake'},\n",
       " 'fl': 'adjective',\n",
       " 'def': [{'sseq': [[['sense',\n",
       "      {'sn': '1',\n",
       "       'dt': [['text',\n",
       "         'being such in appearance only and made with or manufactured from usually cheaper materials '],\n",
       "        ['vis',\n",
       "         [{'t': \"opposed to the unnecessary killing of animals, she'll consider wearing only {it}fake{/it} furs\"}]]],\n",
       "       'syn_list': [[{'wd': 'artificial'},\n",
       "         {'wd': 'bogus'},\n",
       "         {'wd': 'dummy'},\n",
       "         {'wd': 'ersatz'},\n",
       "         {'wd': 'factitious'},\n",
       "         {'wd': 'false'},\n",
       "         {'wd': 'faux'},\n",
       "         {'wd': 'imitation'},\n",
       "         {'wd': 'imitative'},\n",
       "         {'wd': 'man-made'},\n",
       "         {'wd': 'mimic'},\n",
       "         {'wd': 'mock'},\n",
       "         {'wd': 'pretend'},\n",
       "         {'wd': 'sham'},\n",
       "         {'wd': 'simulated'},\n",
       "         {'wd': 'substitute'},\n",
       "         {'wd': 'synthetic'}]],\n",
       "       'rel_list': [[{'wd': 'cultured'},\n",
       "         {'wd': 'manufactured'},\n",
       "         {'wd': 'process'}],\n",
       "        [{'wd': 'unauthentic'}],\n",
       "        [{'wd': 'adulterated'},\n",
       "         {'wd': 'designer'},\n",
       "         {'wd': 'doctored'},\n",
       "         {'wd': 'engineered'},\n",
       "         {'wd': 'fudged'},\n",
       "         {'wd': 'juggled'},\n",
       "         {'wd': 'manipulated'},\n",
       "         {'wd': 'tampered (with)'}],\n",
       "        [{'wd': 'concocted'}, {'wd': 'fabricated'}],\n",
       "        [{'wd': 'counterfeit'},\n",
       "         {'wd': 'deceptive'},\n",
       "         {'wd': 'forged'},\n",
       "         {'wd': 'fraudulent'},\n",
       "         {'wd': 'misleading'},\n",
       "         {'wd': 'phony', 'wvrs': [{'wvl': 'also', 'wva': 'phoney'}]}],\n",
       "        [{'wd': 'affected'},\n",
       "         {'wd': 'brummagem'},\n",
       "         {'wd': 'feigned'},\n",
       "         {'wd': 'pinchbeck'},\n",
       "         {'wd': 'pseudo'},\n",
       "         {'wd': 'spurious'}]],\n",
       "       'near_list': [[{'wd': 'authentic'},\n",
       "         {'wd': 'bona fide'},\n",
       "         {'wd': 'legitimate'},\n",
       "         {'wd': 'true'}],\n",
       "        [{'wd': 'premium'}, {'wd': 'quality'}, {'wd': 'valuable'}],\n",
       "        [{'wd': 'pure'}, {'wd': 'unadulterated'}]],\n",
       "       'ant_list': [[{'wd': 'genuine'}, {'wd': 'natural'}, {'wd': 'real'}]]}]],\n",
       "    [['sense',\n",
       "      {'sn': '2',\n",
       "       'dt': [['text',\n",
       "         'being such in appearance only and made or manufactured with the intention of committing fraud '],\n",
       "        ['vis',\n",
       "         [{'t': 'arrested for peddling {ldquo}designer{rdquo} watches that were {it}fake{/it}'}]]],\n",
       "       'syn_list': [[{'wd': 'bogus'},\n",
       "         {'wd': 'counterfeit'},\n",
       "         {'wd': 'false'},\n",
       "         {'wd': 'forged'},\n",
       "         {'wd': 'inauthentic'},\n",
       "         {'wd': 'phony', 'wvrs': [{'wvl': 'also', 'wva': 'phoney'}]},\n",
       "         {'wd': 'queer'},\n",
       "         {'wd': 'sham'},\n",
       "         {'wd': 'snide'},\n",
       "         {'wd': 'spurious'},\n",
       "         {'wd': 'unauthentic'}]],\n",
       "       'rel_list': [[{'wd': 'artificial'},\n",
       "         {'wd': 'factitious'},\n",
       "         {'wd': 'imitation'},\n",
       "         {'wd': 'man-made'},\n",
       "         {'wd': 'mimic'},\n",
       "         {'wd': 'mock'},\n",
       "         {'wd': 'simulated'},\n",
       "         {'wd': 'substitute'},\n",
       "         {'wd': 'synthetic'}],\n",
       "        [{'wd': 'dummy'}, {'wd': 'nonfunctioning'}, {'wd': 'ornamental'}],\n",
       "        [{'wd': 'cultured'}, {'wd': 'fabricated'}, {'wd': 'manufactured'}],\n",
       "        [{'wd': 'deceptive'}, {'wd': 'delusive'}, {'wd': 'misleading'}]],\n",
       "       'near_list': [[{'wd': 'natural'}],\n",
       "        [{'wd': 'actual'}, {'wd': 'true'}, {'wd': 'valid'}]],\n",
       "       'ant_list': [[{'wd': 'authentic'},\n",
       "         {'wd': 'bona fide'},\n",
       "         {'wd': 'genuine'},\n",
       "         {'wd': 'real'},\n",
       "         {'wd': 'unfaked'}]]}]],\n",
       "    [['sense',\n",
       "      {'sn': '3',\n",
       "       'dt': [['text', 'lacking in natural or spontaneous quality '],\n",
       "        ['vis',\n",
       "         [{'t': \"the boss's pitiful attempts at humor were met with {it}fake{/it} laughter\"}]]],\n",
       "       'syn_list': [[{'wd': 'affected'},\n",
       "         {'wd': 'artificial'},\n",
       "         {'wd': 'assumed'},\n",
       "         {'wd': 'bogus'},\n",
       "         {'wd': 'contrived'},\n",
       "         {'wd': 'factitious'},\n",
       "         {'wd': 'false'},\n",
       "         {'wd': 'feigned'},\n",
       "         {'wd': 'forced'},\n",
       "         {'wd': 'mechanical'},\n",
       "         {'wd': 'mock'},\n",
       "         {'wd': 'phony', 'wvrs': [{'wvl': 'also', 'wva': 'phoney'}]},\n",
       "         {'wd': 'plastic'},\n",
       "         {'wd': 'pretended'},\n",
       "         {'wd': 'pseudo'},\n",
       "         {'wd': 'put-on'},\n",
       "         {'wd': 'sham'},\n",
       "         {'wd': 'simulated'},\n",
       "         {'wd': 'spurious'},\n",
       "         {'wd': 'strained'},\n",
       "         {'wd': 'unnatural'}]],\n",
       "       'rel_list': [[{'wd': 'automatic'},\n",
       "         {'wd': 'canned'},\n",
       "         {'wd': 'concocted'},\n",
       "         {'wd': 'fabricated'},\n",
       "         {'wd': 'hokey'},\n",
       "         {'wd': 'labored'},\n",
       "         {'wd': 'manufactured'},\n",
       "         {'wd': 'pat'},\n",
       "         {'wd': 'unauthentic'},\n",
       "         {'wd': 'unreal'},\n",
       "         {'wd': 'unrealistic'}],\n",
       "        [{'wd': 'double-dealing'},\n",
       "         {'wd': 'empty'},\n",
       "         {'wd': 'facile'},\n",
       "         {'wd': 'hollow'},\n",
       "         {'wd': 'hypocritical'},\n",
       "         {'wd': 'insincere'},\n",
       "         {'wd': 'left-handed'},\n",
       "         {'wd': 'mealy'},\n",
       "         {'wd': 'mealymouthed'},\n",
       "         {'wd': 'two-faced'},\n",
       "         {'wd': 'unctuous'}],\n",
       "        [{'wd': 'exaggerated'},\n",
       "         {'wd': 'histrionic'},\n",
       "         {'wd': 'melodramatic'},\n",
       "         {'wd': 'overacted'},\n",
       "         {'wd': 'overdone'},\n",
       "         {'wd': 'theatrical', 'wvrs': [{'wvl': 'also', 'wva': 'theatric'}]}],\n",
       "        [{'wd': 'cute'},\n",
       "         {'wd': 'cutesy'},\n",
       "         {'wd': 'genteel'},\n",
       "         {'wd': 'goody-goody'},\n",
       "         {'wd': 'mincing'},\n",
       "         {'wd': 'overrefined'},\n",
       "         {'wd': 'simpering'}],\n",
       "        [{'wd': 'conventional'},\n",
       "         {'wd': 'formal'},\n",
       "         {'wd': 'impersonal'},\n",
       "         {'wd': 'inflexible'},\n",
       "         {'wd': 'rigid'},\n",
       "         {'wd': 'stiff'},\n",
       "         {'wd': 'stylized'},\n",
       "         {'wd': 'wooden'}],\n",
       "        [{'wd': 'artful'},\n",
       "         {'wd': 'calculated'},\n",
       "         {'wd': 'conscious'},\n",
       "         {'wd': 'cultivated'},\n",
       "         {'wd': 'deliberate'},\n",
       "         {'wd': 'premeditated'},\n",
       "         {'wd': 'studied'}]],\n",
       "       'near_list': [[{'wd': 'authentic'},\n",
       "         {'wd': 'bona fide'},\n",
       "         {'wd': 'real'},\n",
       "         {'wd': 'realistic'},\n",
       "         {'wd': 'right'},\n",
       "         {'wd': 'true'}],\n",
       "        [{'wd': 'honest'},\n",
       "         {'wd': 'ingenuous'},\n",
       "         {'wd': 'sincere'},\n",
       "         {'wd': 'unpretending'}],\n",
       "        [{'wd': 'easy'}, {'wd': 'effortless'}, {'wd': 'smooth'}],\n",
       "        [{'wd': 'extemporaneous'},\n",
       "         {'wd': 'impromptu'},\n",
       "         {'wd': 'impulsive'},\n",
       "         {'wd': 'instinctive'},\n",
       "         {'wd': 'unconscious'},\n",
       "         {'wd': 'unprompted'},\n",
       "         {'wd': 'unrehearsed'},\n",
       "         {'wd': 'unstudied'}]],\n",
       "       'ant_list': [[{'wd': 'artless'},\n",
       "         {'wd': 'genuine'},\n",
       "         {'wd': 'natural'},\n",
       "         {'wd': 'spontaneous'},\n",
       "         {'wd': 'unaffected'},\n",
       "         {'wd': 'uncontrived'},\n",
       "         {'wd': 'unfeigned'},\n",
       "         {'wd': 'unforced'}]]}]],\n",
       "    [['sense',\n",
       "      {'sn': '4',\n",
       "       'dt': [['text',\n",
       "         'not being or expressing what one appears to be or express '],\n",
       "        ['vis',\n",
       "         [{'t': 'the {it}fake{/it} friendliness of the sales rep made me want to gag'}]]],\n",
       "       'syn_list': [[{'wd': 'artificial'},\n",
       "         {'wd': 'backhanded'},\n",
       "         {'wd': 'counterfeit'},\n",
       "         {'wd': 'double'},\n",
       "         {'wd': 'double-dealing'},\n",
       "         {'wd': 'double-faced'},\n",
       "         {'wd': 'feigned'},\n",
       "         {'wd': 'hypocritical'},\n",
       "         {'wd': 'insincere'},\n",
       "         {'wd': 'Janus-faced'},\n",
       "         {'wd': 'jive', 'wsls': ['slang']},\n",
       "         {'wd': 'left-handed'},\n",
       "         {'wd': 'lip'},\n",
       "         {'wd': 'mealy'},\n",
       "         {'wd': 'mealymouthed'},\n",
       "         {'wd': 'Pecksniffian'},\n",
       "         {'wd': 'phony', 'wvrs': [{'wvl': 'also', 'wva': 'phoney'}]},\n",
       "         {'wd': 'phony-baloney',\n",
       "          'wvrs': [{'wvl': 'or', 'wva': 'phoney-baloney'}]},\n",
       "         {'wd': 'pretended'},\n",
       "         {'wd': 'two-faced'},\n",
       "         {'wd': 'unctuous'}]],\n",
       "       'rel_list': [[{'wd': 'affected'},\n",
       "         {'wd': 'assumed'},\n",
       "         {'wd': 'claptrap'},\n",
       "         {'wd': 'contrived'},\n",
       "         {'wd': 'forced'},\n",
       "         {'wd': 'mechanical'},\n",
       "         {'wd': 'put-on'},\n",
       "         {'wd': 'simulated'},\n",
       "         {'wd': 'strained'},\n",
       "         {'wd': 'unnatural'}],\n",
       "        [{'wd': 'empty'}, {'wd': 'hollow'}, {'wd': 'meaningless'}],\n",
       "        [{'wd': 'deceitful'},\n",
       "         {'wd': 'devious'},\n",
       "         {'wd': 'dishonest'},\n",
       "         {'wd': 'false'},\n",
       "         {'wd': 'untruthful'}],\n",
       "        [{'wd': 'facile'}, {'wd': 'glib'}, {'wd': 'superficial'}],\n",
       "        [{'wd': 'bogus'}, {'wd': 'sham'}],\n",
       "        [{'wd': 'campy'},\n",
       "         {'wd': 'facetious'},\n",
       "         {'wd': 'jocular'},\n",
       "         {'wd': 'tongue-in-cheek'}],\n",
       "        [{'wd': 'canting'},\n",
       "         {'wd': 'pharisaical'},\n",
       "         {'wd': 'pious'},\n",
       "         {'wd': 'sanctimonious'},\n",
       "         {'wd': 'self-righteous'},\n",
       "         {'wd': 'simon-pure'}]],\n",
       "       'near_list': [[{'wd': 'direct'},\n",
       "         {'wd': 'forthright'},\n",
       "         {'wd': 'frank'},\n",
       "         {'wd': 'heart-to-heart'},\n",
       "         {'wd': 'open'},\n",
       "         {'wd': 'plain'},\n",
       "         {'wd': 'straightforward'}]],\n",
       "       'ant_list': [[{'wd': 'artless'},\n",
       "         {'wd': 'candid'},\n",
       "         {'wd': 'genuine'},\n",
       "         {'wd': 'heartfelt'},\n",
       "         {'wd': 'honest'},\n",
       "         {'wd': 'sincere'},\n",
       "         {'wd': 'undesigning'},\n",
       "         {'wd': 'unfeigned'}]]}]]]}],\n",
       " 'shortdef': ['being such in appearance only and made with or manufactured from usually cheaper materials',\n",
       "  'being such in appearance only and made or manufactured with the intention of committing fraud',\n",
       "  'lacking in natural or spontaneous quality']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a= json.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['artificial', 'bogus', 'dummy', 'ersatz', 'factitious', 'false', 'faux', 'imitation', 'imitative', 'man-made', 'mimic', 'mock', 'pretend', 'sham', 'simulated', 'substitute', 'synthetic'], ['bogus', 'counterfeit', 'false', 'forged', 'inauthentic', 'phony', 'queer', 'sham', 'snide', 'spurious', 'unauthentic'], ['affected', 'artificial', 'assumed', 'bogus', 'contrived', 'factitious', 'false', 'feigned', 'forced', 'mechanical', 'mock', 'phony', 'plastic', 'pretended', 'pseudo', 'put-on', 'sham', 'simulated', 'spurious', 'strained', 'unnatural'], ['artificial', 'backhanded', 'counterfeit', 'double', 'double-dealing', 'double-faced', 'feigned', 'hypocritical', 'insincere', 'Janus-faced', 'jive', 'left-handed', 'lip', 'mealy', 'mealymouthed', 'Pecksniffian', 'phony', 'phony-baloney', 'pretended', 'two-faced', 'unctuous']]\n"
     ]
    }
   ],
   "source": [
    "print(a[0]['meta']['syns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = \n",
    "fake_syns =['artificial', 'bogus', 'dummy', 'ersatz', 'factitious', 'false', 'faux', 'imitation', 'imitative', 'man-made', 'mimic', 'mock', 'pretend', 'sham', 'simulated', 'substitute', 'synthetic', 'bogus', 'counterfeit', 'false', 'forged', 'inauthentic', 'phony', 'queer', 'sham', 'snide', 'spurious', 'unauthentic', 'affected', 'artificial', 'assumed', 'bogus', 'contrived', 'factitious', 'false', 'feigned', 'forced', 'mechanical', 'mock', 'phony', 'plastic', 'pretended', 'pseudo', 'put-on', 'sham', 'simulated', 'spurious', 'strained', 'unnatural', 'artificial', 'backhanded', 'counterfeit', 'double', 'double-dealing', 'double-faced', 'feigned', 'hypocritical', 'insincere', 'Janus-faced', 'jive', 'left-handed', 'lip', 'mealy', 'mealymouthed', 'Pecksniffian', 'phony', 'phony-baloney', 'pretended', 'two-faced', 'unctuous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial',\n",
       " 'bogus',\n",
       " 'dummy',\n",
       " 'ersatz',\n",
       " 'factitious',\n",
       " 'false',\n",
       " 'faux',\n",
       " 'imitation',\n",
       " 'imitative',\n",
       " 'man-made',\n",
       " 'mimic',\n",
       " 'mock',\n",
       " 'pretend',\n",
       " 'sham',\n",
       " 'simulated',\n",
       " 'substitute',\n",
       " 'synthetic',\n",
       " 'bogus',\n",
       " 'counterfeit',\n",
       " 'false',\n",
       " 'forged',\n",
       " 'inauthentic',\n",
       " 'phony',\n",
       " 'queer',\n",
       " 'sham',\n",
       " 'snide',\n",
       " 'spurious',\n",
       " 'unauthentic',\n",
       " 'affected',\n",
       " 'artificial',\n",
       " 'assumed',\n",
       " 'bogus',\n",
       " 'contrived',\n",
       " 'factitious',\n",
       " 'false',\n",
       " 'feigned',\n",
       " 'forced',\n",
       " 'mechanical',\n",
       " 'mock',\n",
       " 'phony',\n",
       " 'plastic',\n",
       " 'pretended',\n",
       " 'pseudo',\n",
       " 'put-on',\n",
       " 'sham',\n",
       " 'simulated',\n",
       " 'spurious',\n",
       " 'strained',\n",
       " 'unnatural',\n",
       " 'artificial',\n",
       " 'backhanded',\n",
       " 'counterfeit',\n",
       " 'double',\n",
       " 'double-dealing',\n",
       " 'double-faced',\n",
       " 'feigned',\n",
       " 'hypocritical',\n",
       " 'insincere',\n",
       " 'Janus-faced',\n",
       " 'jive',\n",
       " 'left-handed',\n",
       " 'lip',\n",
       " 'mealy',\n",
       " 'mealymouthed',\n",
       " 'Pecksniffian',\n",
       " 'phony',\n",
       " 'phony-baloney',\n",
       " 'pretended',\n",
       " 'two-faced',\n",
       " 'unctuous']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is relevant to the topic 'artificial'.\n",
      "The sentence is relevant to the topic 'bogus'.\n",
      "The sentence is relevant to the topic 'dummy'.\n",
      "The sentence is relevant to the topic 'ersatz'.\n",
      "The sentence is relevant to the topic 'factitious'.\n",
      "The sentence is relevant to the topic 'false'.\n",
      "The sentence is relevant to the topic 'faux'.\n",
      "The sentence is relevant to the topic 'imitation'.\n",
      "The sentence is relevant to the topic 'imitative'.\n",
      "The sentence is relevant to the topic 'man-made'.\n",
      "The sentence is relevant to the topic 'mimic'.\n",
      "The sentence is relevant to the topic 'mock'.\n",
      "The sentence is relevant to the topic 'pretend'.\n",
      "The sentence is relevant to the topic 'sham'.\n",
      "The sentence is relevant to the topic 'simulated'.\n",
      "The sentence is relevant to the topic 'substitute'.\n",
      "The sentence is relevant to the topic 'synthetic'.\n",
      "The sentence is relevant to the topic 'bogus'.\n",
      "The sentence is relevant to the topic 'counterfeit'.\n",
      "The sentence is relevant to the topic 'false'.\n",
      "The sentence is relevant to the topic 'forged'.\n",
      "The sentence is relevant to the topic 'inauthentic'.\n",
      "The sentence is relevant to the topic 'phony'.\n",
      "The sentence is relevant to the topic 'queer'.\n",
      "The sentence is relevant to the topic 'sham'.\n",
      "The sentence is relevant to the topic 'snide'.\n",
      "The sentence is relevant to the topic 'spurious'.\n",
      "The sentence is relevant to the topic 'unauthentic'.\n",
      "The sentence is relevant to the topic 'affected'.\n",
      "The sentence is relevant to the topic 'artificial'.\n",
      "The sentence is relevant to the topic 'assumed'.\n",
      "The sentence is relevant to the topic 'bogus'.\n",
      "The sentence is relevant to the topic 'contrived'.\n",
      "The sentence is relevant to the topic 'factitious'.\n",
      "The sentence is relevant to the topic 'false'.\n",
      "The sentence is relevant to the topic 'feigned'.\n",
      "The sentence is relevant to the topic 'forced'.\n",
      "The sentence is relevant to the topic 'mechanical'.\n",
      "The sentence is relevant to the topic 'mock'.\n",
      "The sentence is relevant to the topic 'phony'.\n",
      "The sentence is relevant to the topic 'plastic'.\n",
      "The sentence is relevant to the topic 'pretended'.\n",
      "The sentence is relevant to the topic 'pseudo'.\n",
      "The sentence is relevant to the topic 'put-on'.\n",
      "The sentence is relevant to the topic 'sham'.\n",
      "The sentence is relevant to the topic 'simulated'.\n",
      "The sentence is relevant to the topic 'spurious'.\n",
      "The sentence is relevant to the topic 'strained'.\n",
      "The sentence is relevant to the topic 'unnatural'.\n",
      "The sentence is relevant to the topic 'artificial'.\n",
      "The sentence is relevant to the topic 'backhanded'.\n",
      "The sentence is relevant to the topic 'counterfeit'.\n",
      "The sentence is relevant to the topic 'double'.\n",
      "The sentence is relevant to the topic 'double-dealing'.\n",
      "The sentence is relevant to the topic 'double-faced'.\n",
      "The sentence is relevant to the topic 'feigned'.\n",
      "The sentence is relevant to the topic 'hypocritical'.\n",
      "The sentence is relevant to the topic 'insincere'.\n",
      "The sentence is relevant to the topic 'Janus-faced'.\n",
      "The sentence is relevant to the topic 'jive'.\n",
      "The sentence is relevant to the topic 'left-handed'.\n",
      "The sentence is relevant to the topic 'lip'.\n",
      "The sentence is relevant to the topic 'mealy'.\n",
      "The sentence is relevant to the topic 'mealymouthed'.\n",
      "The sentence is relevant to the topic 'Pecksniffian'.\n",
      "The sentence is relevant to the topic 'phony'.\n",
      "The sentence is relevant to the topic 'phony-baloney'.\n",
      "The sentence is relevant to the topic 'pretended'.\n",
      "The sentence is relevant to the topic 'two-faced'.\n",
      "The sentence is relevant to the topic 'unctuous'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "# load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# loop over the topics and check if the sentence is relevant to each one\n",
    "# define the topics and sentence to check\n",
    "# topics = [\"artificial intelligence\", \"machine learning\", \"natural language processing\"]\n",
    "# sentence = \"Machine learning is a subfield of artificial intelligence.\"\n",
    "# topics = ['misleading reviews', 'misleading rating', 'fake rating', 'fake review']\n",
    "topics = fake_syns\n",
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# topics = ['misleading', 'fake review']\n",
    "# sentence= 'just read it realms of layers etc i like it only one of a kind'\n",
    "# sentence = 'Utter garbage. Take the $5 to $7 you plan to spend on this \"tool\" and burn it if you feel the desire to make this purchase. You will never be able to make a reproachable line because a) the two arms move out of plane from each other, b) the screw assembly slips constantly, c) the screw assemble actually runs through a slot rather than a hole, meaning that even when the nut is now moving, you have a 1/4\" wiggle in the scribe line. The positive reviews HAVE TO BE fakes. This thing should not even be in a middle school math class.'\n",
    "# sentence=  'The tape on these NEVER stays in the track. The tape is always coming off and has to be re positioned constantly.Not worth your time and after having bought 2 boxes I opted to throw the rest out rather than suffer thru using them any more. Total garbage.'\n",
    "# sentence = 'I have experienced all of the failure modes listed in these reviews - the printer james often, and in unpredictable ways, the cartridges run out faster than you can believe, and the wireless drops off at the most inopportune times.  I have had to run to a local copy shop to make really critical prints when the cartridge ran out when I needed copies for an important early meeting.Customer service could not be more indifferent.  I actually came to Amazon today to read the reviews on other printers because I am just trashing my Samsung.  I was furious with HP over their ink prices; however, they are not looking so bad now.  Is there a reliablbe, cost efficient printer anywhere?  All those positive reviews for this model makes me question all positive reviews anywhere.I feel like such a sucker for buying this thing.'\n",
    "# sentence = 'Ok I dont think the 5 star reviews on this product are honest. The set is extremely cheap. The case is flimsy and the compass is not well built. I doubt this will serve the needs of my son. I am generally very good at confirming 5 star reviews but fell for it hard this time around. Buyer beware!'\n",
    "# sentence = 'I only hope that this was a quality assurance rather than a deliberate change of Avery product specifications. I have used these protectors for a at least 5 years and they were perfect. Now, I can barely squeeze in a single page and it buckles up (see photos by other reviewers). My last order now goes into garbage, just as I am looking for an alternative product.'\n",
    "# sentence = 'When I purchased these, it was advertised as 2011, which I needed. However, when I received them, they were for 2012. They have no contact info listed and when I reviewed these the first time, it wasnt accepted. I guess they dont like negative feedback. Secondary, I havent heard anything from them to replace .'\n",
    "# sentence = 'Colors are NOT as shown in picture… I received darker colors - just like some of the other reviewers.'\n",
    "# sent = 'I cant find anywhere else to get customer support, so Im reporting my issue here as a 1 star review.  If E-Z Ink can help me get this resolved, I will adjust the review.  I purchased this toner cartridge for a Samsung M2885FW printer.  When I install it in the printer, the printer gives me an error message saying toner not compatible with printer.'\n",
    "# sentt= 'Incompatible. Instructions to override useless.  Sent back in recycle bag provided.  Printer manufacturers got you by the cajones.  I should give one star, but showed mercy.'\n",
    "sentt = ' Docking 4 stars because there is not any support for the printer.'\n",
    "for topic in topics:\n",
    "    # tokenize the sentence and topic using the tokenizer\n",
    "    inputs = tokenizer.encode_plus(sentt, topic, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    # use the model to generate a prediction for the relevance of the sentence to the topic\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "    # print the prediction (0 for not relevant, 1 for relevant)\n",
    "    if prediction == 1:\n",
    "        print(f\"The sentence is relevant to the topic '{topic}'.\")\n",
    "    else:\n",
    "        print(f\"The sentence is not relevant to the topic '{topic}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
