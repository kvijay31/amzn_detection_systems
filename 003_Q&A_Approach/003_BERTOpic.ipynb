{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: \n",
    "\n",
    "Experiment and Investigate BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import json \n",
    "import os \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import gensim, spacy, logging, warnings\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "from typing import Dict, Any, Union\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser']) \n",
    "# added these additional words to stopword list as after first round of pre-processing, I checked into the frequency of words in our corpus. When looking into the corpus\n",
    "# of words, domain specific words such as train, customer and service which occured amongst the top 200 frequent words can be removed because they do not add any additonal \n",
    "# value in extraction of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.1 s, sys: 182 ms, total: 5.28 s\n",
      "Wall time: 5.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ner_dict,date_list, gpe_list = get_named_entity_word_counts(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be2e4add384fe2bd7f01caaa2f3740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4833881"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # utilised sentence transformer to convert text into high-dimensional vector represenatation \n",
    "embeddings = sentence_model.encode(train_reviews_df['final_text'], show_progress_bar=True) # generate sentence embeddings \n",
    "\n",
    "# Train our topic model using our pre-trained sentence-transformers embeddings\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=sentence_model,\n",
    "                       language=\"english\", \n",
    "                       calculate_probabilities=True, \n",
    "                       nr_topics= \"auto\", \n",
    "                       top_n_words= 30, \n",
    "                       n_gram_range=(1,2)\n",
    "                       )\n",
    "topics, probs = topic_model.fit_transform(train_reviews_df['final_text'], embeddings)\n",
    "\n",
    "# Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
    "umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
    "indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "X = umap_embeddings[np.array(indices)]\n",
    "labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_score(X, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intertopic distance\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_adj(text):\n",
    "    \"\"\"\n",
    "    Extracts all the adjectives from the given text and returns them as a string.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to extract adjectives from.\n",
    "\n",
    "    Returns:\n",
    "    str: A string containing all the adjectives in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract all the adjectives from the text\n",
    "    adj = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "\n",
    "    # Join the adjectives into a single string\n",
    "    return ' '.join(adj)\n",
    "# keeping only adjectives, to remove noise from reviews that needed further investigation. Adjectives provide good insights into the customers emotions. \n",
    "train_reviews_2['adj']= train_reviews_2['final_text'].apply(lambda x: keep_adj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7b3253bc64e529940f328581bc962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.69791675"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(train_reviews_2['adj'], show_progress_bar=True)\n",
    "\n",
    "# Train our topic model using our pre-trained sentence-transformers embeddings\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=sentence_model,\n",
    "                       language=\"english\", \n",
    "                       calculate_probabilities=True, \n",
    "                       nr_topics= \"auto\", \n",
    "                       top_n_words= 30, \n",
    "                       n_gram_range=(1,2)\n",
    "                       )\n",
    "topics, probs = topic_model.fit_transform(train_reviews_2['adj'], embeddings)\n",
    "\n",
    "# Generate `X` and `labels` only for non-outlier topics (as they are technically not clusters)\n",
    "umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
    "indices = [index for index, topic in enumerate(topics) if topic != -1]\n",
    "X = umap_embeddings[np.array(indices)]\n",
    "labels = [topic for index, topic in enumerate(topics) if topic != -1]\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_score(X, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Notebook\n"
     ]
    }
   ],
   "source": [
    "print('End of Notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
