{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: \n",
    "\n",
    "Data augmentation for rating management explicit and disagreement with ratings training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "tqdm.pandas()\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_df = pd.read_csv('/Users/kartikvijay/Documents/MADS/Thesis pt.2/augmentation/train_dataset_disagreement_with_ratings.csv')\n",
    "rating_management_df = pd.read_csv('/Users/kartikvijay/Documents/MADS/Thesis pt.2/augmentation/training_rating_management_explicit_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 29, 35,  0, 76, 45, 71, 11, 14, 87, 26, 26, 86, 52, 86,  5, 30,\n",
       "       51, 92, 95, 17,  6,  2, 36, 65, 80, 85, 35, 39, 89, 13, 52, 49, 86,\n",
       "       98, 69, 73, 76, 25, 84, 12, 92, 51, 97, 20, 72, 81, 81, 90, 91, 11,\n",
       "       32, 17, 12, 65, 72, 35, 87,  7, 66, 74, 71, 19, 74, 70, 23, 85, 11,\n",
       "       11, 70, 55, 78, 65, 28, 96, 44, 87, 47, 54, 95, 73, 78, 73, 81, 57,\n",
       "       62, 17, 31, 95, 41, 79, 77, 17, 48, 11, 95, 22, 76, 17, 27, 44, 59,\n",
       "       11, 92, 64, 63, 45,  1, 19, 25, 63, 26, 72, 61,  6, 79, 12, 25, 63,\n",
       "       52, 79,  1, 73, 26, 60,  2, 83,  9,  0, 73, 58, 98, 48, 38, 51, 23,\n",
       "        7, 12, 39, 69, 84, 37, 15, 29, 16, 21, 70, 11, 63, 71, 51, 24, 58,\n",
       "       67, 53, 64, 98, 22, 30, 54, 29, 59, 38, 12, 31, 23, 66, 65, 84, 61,\n",
       "       81, 12, 12, 44, 33, 72, 44, 68, 51, 96, 52, 56, 65,  0, 40, 43, 11,\n",
       "       72,  3, 58, 77, 67, 21, 82, 52,  4, 92, 73, 25, 78])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(0, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the ContextualWordEmbsAug class from the nlpaug library to perform text augmentation. Specifically, it is creating two instances of the ContextualWordEmbsAug class:\n",
    "\n",
    "# augmenter_sub is initialized with the model path 'roberta-base' and the action \"substitute\". This means that it will use the RoBERTa model to substitute words in the text with similar words.\n",
    "\n",
    "# augmenter_insert is initialized with the model path 'roberta-base' and the action \"insert\". This means that it will use the RoBERTa model to insert words into the text.\n",
    "\n",
    "# These augmenters can be used to generate variations of text data by replacing or inserting words using the contextual information from the RoBERTa model.\n",
    "\n",
    "augmenter_sub = aug.ContextualWordEmbsAug(model_path='roberta-base', action=\"substitute\") # experimented but changes meaning of sentence completetely\n",
    "augmenter_insert = aug.ContextualWordEmbsAug(model_path='roberta-base', action=\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence(text, augmenter): \n",
    "    \"\"\"Augments a sentence by substituting or inserting words using the given augmenter.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input sentence to be augmented.\n",
    "        augmenter (nlpaug.augmenter.word.ContextualWordEmbsAug): The augmenter object used for augmentation.\n",
    "\n",
    "    Returns:\n",
    "        str: The augmented sentence.\n",
    "    \"\"\"\n",
    "    # Process the text with the spacy model\n",
    "    doc = nlp(text)\n",
    "    new_sent = ''\n",
    "    for sent in doc.sents: \n",
    "        # begin augmentation of each sentence\n",
    "        # print(sent.text)\n",
    "        # print('-----------------------')\n",
    "        augmented_text= augmenter.augment(sent.text)\n",
    "        print(augmented_text[0])\n",
    "        new_sent= new_sent+ ' '+ augmented_text[0] \n",
    "    return new_sent\n",
    "\n",
    "def augmentMyData(df,label_name, label_val,  augmenter, repetitions=1, samples=200):\n",
    "    \"\"\"Augments the data by generating new samples based on the given augmentation parameters.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the data to be augmented.\n",
    "        label_name (str): The name of the label column in the DataFrame.\n",
    "        label_val: The value of the label for which augmentation is performed.\n",
    "        augmenter (nlpaug.augmenter.word.ContextualWordEmbsAug): The augmenter object used for augmentation.\n",
    "        repetitions (int, optional): The number of times each sentence is augmented. Defaults to 1.\n",
    "        samples (int, optional): The number of samples to be generated from the minority class. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The augmented DataFrame.\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    # select only the minority class samples\n",
    "    spam_df = df[df[label_name] == label_val].reset_index(drop=True) # removes unecessary index column\n",
    "    for i in tqdm(np.random.randint(0, len(spam_df), samples)):\n",
    "        # generating 'n_samples' augmented texts\n",
    "        for _ in range(repetitions):\n",
    "            # the random sentence chosen for augmentation\n",
    "            augmented_text= augment_sentence(spam_df['full_review'].iloc[i], augmenter)\n",
    "            \n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    data = {\n",
    "        label_name: label_val,\n",
    "        'full_review': augmented_texts\n",
    "    }\n",
    "    aug_df = pd.DataFrame(data)\n",
    "    df = shuffle(df.append(aug_df).reset_index(drop=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very very cheaply home made commercial product.\n",
      "I really regret buying even it.\n",
      "Its a not really even worth the 2$. The reviews here are completely and fake.\n",
      "Dont sell buy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This is a very very cheaply home made commercial product. I really regret buying even it. Its a not really even worth the 2$. The reviews here are completely and fake. Dont sell buy.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"This is a very cheaply made product . I regret buying it.Its not even worth 2$. The reviews here are completely fake. Dont buy.\"\n",
    "augment_sentence(test, augmenter_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_management_df_1 = rating_management_df[['full_review', 'rating_managment_explicit_ohe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6191\n",
       "1      97\n",
       "Name: rating_managment_explicit_ohe, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_management_df_1.rating_managment_explicit_ohe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [4:06:39<00:00,  2.96s/it]    \n",
      "/var/folders/cg/93nnwcc94dd7ypq102y4db_c0000gn/T/ipykernel_47810/173017619.py:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = shuffle(df.append(aug_df).reset_index(drop=True))\n"
     ]
    }
   ],
   "source": [
    "rating_management_df_2=augmentMyData(rating_management_df_1,'rating_managment_explicit_ohe', 1, augmenter_insert, samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_management_df_2.to_csv('5000_training_rating_management_explicit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5988\n",
       "1     300\n",
       "Name: disagreement_with_ratings_ohe, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagreement_train_raw_df = pd.read_csv('/Users/kartikvijay/Documents/MADS/Thesis pt.2/augmentation/train_dataset_disagreement_with_ratings.csv')\n",
    "disagreement_train_raw_df.disagreement_with_ratings_ohe.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [2:01:44<00:00,  2.92s/it]   \n",
      "/var/folders/cg/93nnwcc94dd7ypq102y4db_c0000gn/T/ipykernel_47810/173017619.py:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = shuffle(df.append(aug_df).reset_index(drop=True))\n"
     ]
    }
   ],
   "source": [
    "disagreement_train_augmented=augmentMyData(disagreement_train_raw_df,'disagreement_with_ratings_ohe', 1, augmenter_insert, samples=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_train_augmented.to_csv('disagreement_with_ratings_train_augmented_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [2:31:55<00:00,  2.60s/it]  \n",
      "/var/folders/cg/93nnwcc94dd7ypq102y4db_c0000gn/T/ipykernel_47810/173017619.py:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = shuffle(df.append(aug_df).reset_index(drop=True))\n"
     ]
    }
   ],
   "source": [
    "disagreement_train_augmented_1=augmentMyData(disagreement_train_raw_df,'disagreement_with_ratings_ohe', 1, augmenter_insert, samples=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_train_augmented_1.to_csv('3500_augmented_disagreement_with_ratings.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
